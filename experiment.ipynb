{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fab6349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fathimahusna/Downloads/TwistDigital/twistenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define the Result Data Structure\n",
    "@dataclass\n",
    "class ContradictionResult:\n",
    "    has_contradiction: bool\n",
    "    confidence: float\n",
    "    contradicting_pairs: List[Tuple[str, str]]\n",
    "    explanation: str\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99e30a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detector class defined.\n"
     ]
    }
   ],
   "source": [
    "class SemanticContradictionDetector:\n",
    "    \"\"\"\n",
    "    Detects semantic contradictions within a single document using a Cross-Encoder.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"default\"):\n",
    "        # Use the \"Small\" DeBERTa model for speed and accuracy\n",
    "        target_model = \"cross-encoder/nli-deberta-v3-small\" if model_name == \"default\" else model_name\n",
    "        \n",
    "        print(f\"Loading model: {target_model}...\")\n",
    "        self.model = CrossEncoder(target_model, device='cpu')\n",
    "        \n",
    "        # --- ROBUST AUTO-CALIBRATION ---\n",
    "        print(\"Calibrating label mapping...\")\n",
    "        calibration_data = [\n",
    "            (\"The door is open.\", \"The door is closed.\"),       # Hard Contradiction\n",
    "            (\"The cat is sleeping.\", \"The animal is resting.\"), # Hard Entailment\n",
    "        ]\n",
    "        \n",
    "        scores = self.model.predict(calibration_data)\n",
    "        \n",
    "        # Dynamically determine which index is 'Contradiction'\n",
    "        self.contradiction_id = np.argmax(scores[0])\n",
    "        self.entailment_id = np.argmax(scores[1])\n",
    "        \n",
    "        print(f\"Calibration Complete. Contradiction Index: {self.contradiction_id}\")\n",
    "        \n",
    "        # Fallback safety check\n",
    "        if self.contradiction_id == self.entailment_id:\n",
    "            self.contradiction_id = 2 \n",
    "\n",
    "    def _softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    def preprocess(self, text: str) -> List[str]:\n",
    "        text = text.strip()\n",
    "        # Robust splitting by punctuation\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        \n",
    "        cleaned = []\n",
    "        for s in sentences:\n",
    "            s = s.strip()\n",
    "            if len(s) < 5: continue\n",
    "            \n",
    "            # --- CLEAN DISCOURSE MARKERS ---\n",
    "            s = re.sub(r'^(However|But|Although|Yet),?\\s*', '', s, flags=re.IGNORECASE)\n",
    "            s = re.sub(r'\\s*,?\\s*(though|however)\\.?$', '.', s, flags=re.IGNORECASE)\n",
    "            cleaned.append(s)\n",
    "        return cleaned\n",
    "    \n",
    "    def extract_claims(self, sentences: List[str]) -> List[Dict[str, Any]]:\n",
    "        return [{\"id\": i, \"text\": s} for i, s in enumerate(sentences)]\n",
    "    \n",
    "    def check_contradiction(self, claim_a: Dict, claim_b: Dict) -> Tuple[bool, float]:\n",
    "        text_a = claim_a['text']\n",
    "        text_b = claim_b['text']\n",
    "        \n",
    "        # --- BIDIRECTIONAL CHECK ---\n",
    "        inputs = [(text_a, text_b), (text_b, text_a)]\n",
    "        \n",
    "        scores = self.model.predict(inputs)\n",
    "        probs_0 = self._softmax(scores[0])\n",
    "        probs_1 = self._softmax(scores[1])\n",
    "        \n",
    "        score_0 = probs_0[self.contradiction_id]\n",
    "        score_1 = probs_1[self.contradiction_id]\n",
    "        \n",
    "        max_score = max(score_0, score_1)\n",
    "        \n",
    "        # Threshold 0.80 filters out soft errors\n",
    "        return (max_score > 0.80), float(max_score)\n",
    "    \n",
    "    def analyze(self, text: str) -> ContradictionResult:\n",
    "        sentences = self.preprocess(text)\n",
    "        if len(sentences) < 2:\n",
    "            return ContradictionResult(False, 0.0, [], \"Insufficient text.\")\n",
    "            \n",
    "        claims = self.extract_claims(sentences)\n",
    "        contradictions = []\n",
    "        max_confidence = 0.0\n",
    "        \n",
    "        for i in range(len(claims)):\n",
    "            for j in range(i + 1, len(claims)):\n",
    "                if claims[i]['text'] == claims[j]['text']:\n",
    "                    continue\n",
    "                    \n",
    "                is_contra, conf = self.check_contradiction(claims[i], claims[j])\n",
    "                \n",
    "                if is_contra:\n",
    "                    contradictions.append((claims[i]['text'], claims[j]['text']))\n",
    "                    max_confidence = max(max_confidence, conf)\n",
    "        \n",
    "        has_contradiction = len(contradictions) > 0\n",
    "        explanation = f\"Found {len(contradictions)} logical inconsistencies.\" if has_contradiction else \"Consistent.\"\n",
    "        final_conf = max_confidence if has_contradiction else (1.0 - max_confidence)\n",
    "        \n",
    "        return ContradictionResult(\n",
    "            has_contradiction=has_contradiction,\n",
    "            confidence=round(final_conf, 3),\n",
    "            contradicting_pairs=contradictions,\n",
    "            explanation=explanation\n",
    "        )\n",
    "\n",
    "print(\"Detector class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "013e4b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(detector: SemanticContradictionDetector, test_data: List[Dict]) -> Dict[str, float]:\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    print(\"\\n--- Detailed Evaluation ---\")\n",
    "    for item in test_data:\n",
    "        text = item['text']\n",
    "        expected = item['has_contradiction']\n",
    "        \n",
    "        result = detector.analyze(text)\n",
    "        \n",
    "        y_true.append(expected)\n",
    "        y_pred.append(result.has_contradiction)\n",
    "        \n",
    "        status = \"CORRECT\" if expected == result.has_contradiction else \"WRONG\"\n",
    "        print(f\"ID {item.get('id', '?')}: {status} | Pred: {result.has_contradiction} | True: {expected}\")\n",
    "        \n",
    "        if result.has_contradiction:\n",
    "             print(f\"  > Detected: {result.contradicting_pairs[0]}\")\n",
    "        elif status == \"WRONG\":\n",
    "             print(f\"  > Fail Context: {text[:60]}...\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "\n",
    "print(\"Evaluation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "587c9944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: cross-encoder/nli-deberta-v3-small...\n",
      "Calibrating label mapping...\n",
      "Calibration Complete. Contradiction Index: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize detector\n",
    "detector = SemanticContradictionDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f022c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 samples from dataset.txt\n",
      "\n",
      "Example Text:\n",
      "This laptop is incredibly fast. Boot time is under 10 seconds. However, I find myself waiting 5 minutes just to open Chrome. The performance is unmatched in this price range.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('dataset.txt', 'r') as f:\n",
    "        content = f.read()\n",
    "        SAMPLE_REVIEWS = ast.literal_eval(content)\n",
    "        print(f\"Loaded {len(SAMPLE_REVIEWS)} samples from dataset.txt\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'dataset.txt' not found. Using fallback sample data.\")\n",
    "    SAMPLE_REVIEWS = [\n",
    "        {\"id\": 1, \"text\": \"This laptop is fast. However, I wait 5 mins for chrome.\", \"has_contradiction\": True},\n",
    "        {\"id\": 2, \"text\": \"Great camera. Good photos.\", \"has_contradiction\": False},\n",
    "        {\"id\": 3, \"text\": \"Durable phone. Screen cracked immediately.\", \"has_contradiction\": True}\n",
    "    ]\n",
    "\n",
    "# Show the first example\n",
    "print(f\"\\nExample Text:\\n{SAMPLE_REVIEWS[0]['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9762a6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Analysis ---\n",
      "Review 1: True (Conf: 0.998)\n",
      "Review 2: False (Conf: 1.0)\n",
      "Review 3: True (Conf: 1.0)\n",
      "Review 4: True (Conf: 0.999)\n",
      "Review 5: False (Conf: 1.0)\n",
      "Review 6: True (Conf: 1.0)\n",
      "Review 7: True (Conf: 1.0)\n",
      "Review 8: False (Conf: 1.0)\n",
      "\n",
      "--- Detailed Evaluation ---\n",
      "ID 1: CORRECT | Pred: True | True: True\n",
      "  > Detected: ('Boot time is under 10 seconds.', 'I find myself waiting 5 minutes just to open Chrome.')\n",
      "ID 2: CORRECT | Pred: False | True: False\n",
      "ID 3: CORRECT | Pred: True | True: True\n",
      "  > Detected: (\"I've never had a phone this durable.\", 'Dropped it multiple times with no damage.')\n",
      "ID 4: CORRECT | Pred: True | True: True\n",
      "  > Detected: ('Customer service was unhelpful and rude.', 'They resolved my issue within minutes and even gave me a discount.')\n",
      "ID 5: CORRECT | Pred: False | True: False\n",
      "ID 6: CORRECT | Pred: True | True: True\n",
      "  > Detected: ('Shipping was lightning fast - arrived in 2 days.', 'The three-week wait was worth it.')\n",
      "ID 7: CORRECT | Pred: True | True: True\n",
      "  > Detected: ('This blender is whisper quiet.', 'My baby sleeps right through it.')\n",
      "ID 8: CORRECT | Pred: False | True: False\n",
      "\n",
      "Final Metrics: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Run Analysis on all reviews\n",
    "print(\"\\n--- Running Analysis ---\")\n",
    "for review in SAMPLE_REVIEWS:\n",
    "    result = detector.analyze(review[\"text\"])\n",
    "    print(f\"Review {review['id']}: {result.has_contradiction} (Conf: {result.confidence})\")\n",
    "\n",
    "# Calculate Final Metrics\n",
    "metrics = evaluate(detector, SAMPLE_REVIEWS)\n",
    "print(f\"\\nFinal Metrics: {metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twistenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
